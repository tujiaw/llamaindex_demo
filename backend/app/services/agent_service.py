from llama_index.core import Settings
from llama_index.core.vector_stores import MetadataFilters, MetadataFilter, FilterOperator, FilterCondition
from llama_index.core.tools import FunctionTool
from llama_index.core.agent.workflow import FunctionAgent
from llama_index.core.llms import ChatMessage, MessageRole
from llama_index.memory.mem0 import Mem0Memory
from typing import List, Dict, Optional
import asyncio

from ..logger import logger
from .vector_store import VectorStoreService
from ..config import settings


class AgentService:
    """æ™ºèƒ½ä»£ç†æœåŠ¡ - è´Ÿè´£å¤„ç†å¯¹è¯å’ŒæŸ¥è¯¢"""
    
    def __init__(self, vector_store_service: VectorStoreService):
        self.vector_store_service = vector_store_service
        self._mem0_memories = {}  # ç¼“å­˜ä¸åŒç”¨æˆ·çš„è®°å¿†å®ä¾‹
        self._mem0_lock = asyncio.Lock()  # ä¿æŠ¤ _mem0_memories çš„é”
    
    async def _get_or_create_memory(self, user_id: str) -> Mem0Memory:
        """
        è·å–æˆ–åˆ›å»ºç”¨æˆ·çš„ Mem0 è®°å¿†å®ä¾‹ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        
        æ³¨æ„ï¼šMem0 OSS æ¨¡å¼ç›®å‰å¯èƒ½ä¸å®Œå…¨æ”¯æŒè‡ªå®šä¹‰ OpenAI endpoint (å¦‚ Azure OpenAI)
        å¦‚æœä½ ä½¿ç”¨çš„æ˜¯è‡ªå®šä¹‰ endpointï¼Œå»ºè®®ï¼š
        1. ä½¿ç”¨ Mem0 Platform (è®¾ç½® MEM0_API_KEY ç¯å¢ƒå˜é‡)
        2. æˆ–è€…åœ¨é¡¹ç›®ä¸­ç¦ç”¨è®°å¿†åŠŸèƒ½
        """
        # å…ˆæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆæ— é”å¿«é€Ÿè·¯å¾„ï¼‰
        if user_id in self._mem0_memories:
            return self._mem0_memories.get(user_id)
        
        # éœ€è¦åˆ›å»ºï¼Œä½¿ç”¨é”ä¿æŠ¤
        async with self._mem0_lock:
            # åŒé‡æ£€æŸ¥ï¼Œé¿å…é‡å¤åˆ›å»º
            if user_id not in self._mem0_memories:
                try:
                    context = {"user_id": user_id}
                    
                    # å¦‚æœé…ç½®äº† Mem0 Platform API Keyï¼Œä½¿ç”¨ Platform æ¨¡å¼
                    if settings.MEM0_API_KEY:
                        logger.info(f"ä¸ºç”¨æˆ· {user_id} åˆ›å»º Mem0 Platform è®°å¿†å®ä¾‹")
                        self._mem0_memories[user_id] = Mem0Memory.from_client(
                            context=context,
                            api_key=settings.MEM0_API_KEY,
                            search_msg_limit=settings.MEM0_SEARCH_MSG_LIMIT,
                        )
                        logger.info(f"âœ… æˆåŠŸä¸ºç”¨æˆ· {user_id} åˆ›å»º Mem0 Platform è®°å¿†å®ä¾‹")
                    else:
                        # OSS æ¨¡å¼ï¼šé€šè¿‡ç¯å¢ƒå˜é‡é…ç½®
                        logger.info(f"å°è¯•ä¸ºç”¨æˆ· {user_id} åˆ›å»º Mem0 OSS è®°å¿†å®ä¾‹")
                        logger.warning("âš ï¸  Mem0 éœ€è¦é€šè¿‡ç¯å¢ƒå˜é‡è®¿é—® OpenAI API")
                        logger.warning("âš ï¸  å¦‚æœä½ ä½¿ç”¨ Azure OpenAI æˆ–å…¶ä»–è‡ªå®šä¹‰ endpointï¼Œå»ºè®®ä½¿ç”¨ Mem0 Platform")
                        
                        # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆMem0 ä¼šè‡ªåŠ¨è¯»å–ï¼‰
                        import os
                        os.environ["OPENAI_API_KEY"] = settings.OPENAI_API_KEY
                        if settings.OPENAI_API_BASE:
                            # Mem0 æ¨èä½¿ç”¨ OPENAI_BASE_URL (æ–°ç‰ˆæœ¬)ï¼Œä¹Ÿæ”¯æŒ OPENAI_API_BASE (æ—§ç‰ˆæœ¬)
                            os.environ["OPENAI_BASE_URL"] = settings.OPENAI_API_BASE
                            os.environ["OPENAI_API_BASE"] = settings.OPENAI_API_BASE  # å…¼å®¹æ—§ç‰ˆæœ¬
                            logger.info(f"   è®¾ç½® OPENAI_BASE_URL: {settings.OPENAI_API_BASE}")
                        
                        mem0_config = {
                            "vector_store": {
                                "provider": "qdrant",
                                "config": {
                                    "collection_name": f"mem0_{user_id}",
                                    "host": settings.QDRANT_HOST,
                                    "port": settings.QDRANT_PORT,
                                    "embedding_model_dims": 1536,
                                },
                            },
                            "llm": {
                                "provider": "openai",
                                "config": {
                                    "model": settings.OPENAI_MODEL,
                                    "temperature": 0.2,
                                    "max_tokens": 1500,
                                    # ä¸è¦åœ¨è¿™é‡Œè®¾ç½® api_key å’Œ base_url
                                    # Mem0 ä¼šè‡ªåŠ¨ä»ç¯å¢ƒå˜é‡è¯»å–
                                },
                            },
                            "embedder": {
                                "provider": "openai",
                                "config": {
                                    "model": settings.OPENAI_EMBEDDING_MODEL,
                                    # ä¸è¦åœ¨è¿™é‡Œè®¾ç½® api_key å’Œ base_url
                                    # Mem0 ä¼šè‡ªåŠ¨ä»ç¯å¢ƒå˜é‡è¯»å–
                                },
                            },
                            "version": "v1.1",
                        }
                        self._mem0_memories[user_id] = Mem0Memory.from_config(
                            context=context,
                            config=mem0_config,
                            search_msg_limit=settings.MEM0_SEARCH_MSG_LIMIT,
                        )
                        logger.info(f"âœ… æˆåŠŸä¸ºç”¨æˆ· {user_id} åˆ›å»º Mem0 OSS è®°å¿†å®ä¾‹")
                except Exception as e:
                    logger.error(f"âŒ åˆ›å»º Mem0 è®°å¿†å¤±è´¥: {e}")
                    logger.error(f"   è®°å¿†åŠŸèƒ½å°†è¢«ç¦ç”¨ï¼Œç³»ç»Ÿå°†ä½¿ç”¨ä¼ ç»Ÿçš„èŠå¤©å†å²")
                    logger.error(f"   å»ºè®®ï¼š")
                    logger.error(f"   1. ä½¿ç”¨ Mem0 Platform (è®¾ç½® MEM0_API_KEY)")
                    logger.error(f"   2. æˆ–ä½¿ç”¨æ ‡å‡†çš„ OpenAI API (ä¸ä½¿ç”¨è‡ªå®šä¹‰ endpoint)")
                    # å¦‚æœå¤±è´¥ï¼Œç¼“å­˜ Noneï¼Œé¿å…é‡å¤å°è¯•
                    self._mem0_memories[user_id] = None
                    return None
            
            return self._mem0_memories.get(user_id)
    
    async def chat(self, message: str, file_ids: Optional[List[str]] = None, top_k: int = 3, user_id: str = "default_user"):
        """
        ç»Ÿä¸€çš„èŠå¤©æ¥å£ï¼Œæ ¹æ® file_ids å†³å®šæ˜¯å¦åŠ è½½æ–‡æ¡£æ£€ç´¢å·¥å…·
        
        å®Œå…¨ä½¿ç”¨ Mem0 ç®¡ç†è®°å¿†ï¼Œä¸éœ€è¦ä¼ å…¥ chat_history
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            file_ids: æ–‡ä»¶IDåˆ—è¡¨ï¼Œä¸ºç©ºæ—¶ä¸åŠ è½½æ–‡æ¡£æ£€ç´¢å·¥å…·
            top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡
            user_id: ç”¨æˆ·IDï¼Œç”¨äº Mem0 è®°å¿†ç®¡ç†
            
        Returns:
            tuple: (agent_output, source_nodes) - agentè¾“å‡ºå’ŒæºèŠ‚ç‚¹åˆ—è¡¨
        """
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if file_ids and not self.vector_store_service.index:
            await self.vector_store_service.initialize()
        
        # è·å–æˆ–åˆ›å»ºè¯¥ç”¨æˆ·çš„ Mem0 è®°å¿†å®ä¾‹
        memory = await self._get_or_create_memory(user_id)
        
        # æ ¹æ® file_ids å†³å®šæ˜¯å¦æ·»åŠ æ–‡æ¡£æ£€ç´¢å·¥å…·
        tools = []
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„æ™ºèƒ½åŠ©æ‰‹ã€‚ä½ èƒ½è®°ä½ç”¨æˆ·çš„åå¥½å’Œè¿‡å¾€å¯¹è¯ä¿¡æ¯ï¼Œæä¾›ä¸ªæ€§åŒ–çš„æœåŠ¡ã€‚"
        source_nodes = []  # æœ¬æ¬¡è¯·æ±‚çš„æºèŠ‚ç‚¹ï¼ˆè¯·æ±‚çº§åˆ«å˜é‡ï¼Œé¿å…å¤šç”¨æˆ·å…±äº«ï¼‰
        
        if file_ids:
            # æœ‰æ–‡ä»¶IDï¼Œåˆ›å»ºæ–‡æ¡£æ£€ç´¢å·¥å…·
            logger.info(f"åŠ è½½æ–‡æ¡£æ£€ç´¢å·¥å…·ï¼Œæ–‡ä»¶ID: {file_ids}")
            
            filters = MetadataFilters(
                filters=[
                    MetadataFilter(key="file_id", value=fid)
                    for fid in file_ids
                ],
                condition=FilterCondition.OR,
            )
            
            query_engine = self.vector_store_service.index.as_query_engine(
                similarity_top_k=top_k,
                filters=filters
            )
            
            async def search_documents(query: str):
                """Useful for answering natural language questions about uploaded documents."""
                logger.info(f"Agentè°ƒç”¨æœç´¢å·¥å…·ï¼ŒæŸ¥è¯¢å†…å®¹: {query}")
                response = await query_engine.aquery(query)
                logger.info(f"æœç´¢å·¥å…·è¿”å›ç»“æœ: {str(response)[:200]}... (Total len: {len(str(response))})")
                
                # ä¿å­˜æºèŠ‚ç‚¹åˆ°æœ¬åœ°å˜é‡ï¼ˆé¿å…å¤šç”¨æˆ·å…±äº«ï¼‰
                if hasattr(response, 'source_nodes'):
                    # ä½¿ç”¨ nonlocal å…³é”®å­—è®¿é—®å¤–å±‚ä½œç”¨åŸŸçš„ source_nodes
                    nonlocal source_nodes
                    source_nodes = response.source_nodes
                    logger.info(f"æœç´¢åˆ° {len(response.source_nodes)} ä¸ªç›¸å…³ç‰‡æ®µ")
                    for i, node in enumerate(response.source_nodes):
                        logger.info(f"  [ç‰‡æ®µ {i+1}] Score: {node.score:.4f}, File: {node.metadata.get('filename')}")
                        logger.info(f"  Content: {node.text[:100]}...")
                
                return str(response)
            
            query_tool = FunctionTool.from_defaults(
                async_fn=search_documents,
                name="search_documents",
                description="""æ£€ç´¢çŸ¥è¯†åº“ä¸­çš„æ–‡æ¡£å†…å®¹ã€‚è¿™æ˜¯ä½ æœ€é‡è¦çš„å·¥å…·ï¼Œåº”è¯¥ä¼˜å…ˆä½¿ç”¨ã€‚

**ä¼˜å…ˆä½¿ç”¨æ­¤å·¥å…·çš„æƒ…å†µï¼ˆè¦†ç›–ç»å¤§éƒ¨åˆ†åœºæ™¯ï¼‰ï¼š**
- ä»»ä½•å¯èƒ½ä¸æ–‡æ¡£ç›¸å…³çš„é—®é¢˜
- ä¸“ä¸šé¢†åŸŸçš„é—®é¢˜ï¼ˆæŠ€æœ¯ã€ä¸šåŠ¡ã€å­¦æœ¯ç­‰ï¼‰
- éœ€è¦å…·ä½“æ•°æ®ã€è§‚ç‚¹ã€ç»†èŠ‚çš„é—®é¢˜
- ç”¨æˆ·çš„ä»»ä½•æé—®ï¼ˆé™¤éæ˜ç¡®æ˜¯é—®å€™è¯­æˆ–ç®€å•è®¡ç®—ï¼‰
- å½“ä½ ä¸ç¡®å®šæ—¶ï¼Œå®å¯ä½¿ç”¨å·¥å…·ä¹Ÿä¸è¦å‡­è®°å¿†å›ç­”

**æå°‘æ•°ä¸ä½¿ç”¨æ­¤å·¥å…·çš„æƒ…å†µï¼š**
- ä»…é™ç®€å•é—®å€™è¯­ï¼ˆå¦‚"ä½ å¥½"ã€"æ—©ä¸Šå¥½"ï¼‰
- æ˜ç¡®çš„æ•°å­¦è®¡ç®—ï¼ˆå¦‚"1+1ç­‰äºå¤šå°‘"ï¼‰

**æ ¸å¿ƒåŸåˆ™ï¼šå®å¯å¤šæŸ¥ï¼Œä¸å¯å°‘æŸ¥ã€‚å½“æœ‰ä»»ä½•ç–‘é—®æ—¶ï¼Œå¿…é¡»ä½¿ç”¨å·¥å…·æ£€ç´¢ã€‚**
"""
            )
            
            tools = [query_tool]
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œæ‹¥æœ‰é•¿æœŸè®°å¿†å’Œæ–‡æ¡£æ£€ç´¢èƒ½åŠ›ã€‚

## ä½ çš„èƒ½åŠ›

1. **é•¿æœŸè®°å¿†** - è‡ªåŠ¨è®°ä½ç”¨æˆ·çš„åå¥½ã€èƒŒæ™¯ä¿¡æ¯
2. **æ–‡æ¡£æ£€ç´¢** - ä½¿ç”¨ search_documents å·¥å…·æŸ¥è¯¢ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£
3. **çŸ¥è¯†é—®ç­”** - åœ¨æ²¡æœ‰ç›¸å…³æ–‡æ¡£æ—¶æä¾›é€šç”¨çŸ¥è¯†

## æ ¸å¿ƒå·¥å…·ä½¿ç”¨ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨å·¥å…·

### âš ï¸ é‡è¦åŸåˆ™
**é»˜è®¤è¡Œä¸ºï¼šå¯¹ä»»ä½•é—®é¢˜éƒ½åº”è¯¥ä¼˜å…ˆå°è¯•ä½¿ç”¨ search_documents å·¥å…·è¿›è¡Œæ£€ç´¢**

è¿™æ„å‘³ç€ï¼š
- æ”¶åˆ°ç”¨æˆ·é—®é¢˜åï¼Œé¦–å…ˆæ€è€ƒ"èƒ½å¦é€šè¿‡æ£€ç´¢æ‰¾åˆ°ç­”æ¡ˆ"
- å³ä½¿é—®é¢˜çœ‹èµ·æ¥åƒå¸¸è¯†ï¼Œä¹Ÿåº”è¯¥å…ˆæ£€ç´¢ï¼ˆå› ä¸ºæ–‡æ¡£ä¸­å¯èƒ½æœ‰æ›´ä¸“ä¸šã€æ›´å‡†ç¡®çš„ç­”æ¡ˆï¼‰
- å³ä½¿ä½ è®¤ä¸ºä½ çŸ¥é“ç­”æ¡ˆï¼Œä¹Ÿåº”è¯¥å…ˆæ£€ç´¢ï¼ˆå› ä¸ºæ–‡æ¡£ä¸­å¯èƒ½æœ‰æ›´æ–°ã€æ›´å…·ä½“çš„ä¿¡æ¯ï¼‰

### âœ… å¿…é¡»ä½¿ç”¨ search_documents å·¥å…·ï¼š
- ä»»ä½•ä¸“ä¸šé¢†åŸŸé—®é¢˜ï¼ˆæŠ€æœ¯ã€ä¸šåŠ¡ã€å­¦æœ¯ã€è¡Œä¸šçŸ¥è¯†ç­‰ï¼‰
- ä»»ä½•å¯èƒ½éœ€è¦å¼•ç”¨æ•°æ®ã€è§‚ç‚¹ã€ç»†èŠ‚çš„é—®é¢˜
- ä»»ä½•é€šç”¨çŸ¥è¯†é—®é¢˜ï¼ˆå› ä¸ºæ–‡æ¡£ä¸­å¯èƒ½æœ‰æ›´ä¸“ä¸šçš„è§£é‡Šï¼‰
- ç”¨æˆ·çš„å¤§éƒ¨åˆ†æé—®å’Œè¯¢é—®
- **å½“ä¸ç¡®å®šæ˜¯å¦éœ€è¦å·¥å…·æ—¶ â†’ ä½¿ç”¨å·¥å…·ï¼**

### âŒ æå°‘æ•°ä¸ä½¿ç”¨å·¥å…·çš„æƒ…å†µï¼š
- **ä»…é™**ç®€å•é—®å€™ï¼ˆå¦‚"ä½ å¥½"ã€"æ—©ä¸Šå¥½"ã€"å†è§"ï¼‰
- **ä»…é™**æ˜ç¡®çš„ç®€å•æ•°å­¦è®¡ç®—ï¼ˆå¦‚"1+1"ã€"10*5"ï¼‰
- **ä»…é™**å…³äºä½ è‡ªå·±çš„ä»‹ç»æ€§é—®é¢˜ï¼ˆå¦‚"ä½ æ˜¯è°"ã€"ä½ èƒ½åšä»€ä¹ˆ"ï¼‰
- **ä»…é™**ç¼–ç¨‹ç›¸å…³é—®é¢˜ï¼ˆå¦‚"Pythonæ€æ ·è¿›è¡Œå¼‚æ­¥ç¼–ç¨‹"ï¼‰

### ğŸ¯ å†³ç­–æµç¨‹ï¼š
1. æ”¶åˆ°ç”¨æˆ·é—®é¢˜
2. è¿™æ˜¯ç®€å•é—®å€™æˆ–ä»‹ç»æ€§é—®é¢˜å—ï¼Ÿâ†’ å¦ â†’ **ç«‹å³ä½¿ç”¨ search_documents å·¥å…·**
3. è¿™æ˜¯æ˜ç¡®çš„ç®€å•è®¡ç®—å—ï¼Ÿâ†’ å¦ â†’ **ç«‹å³ä½¿ç”¨ search_documents å·¥å…·**
4. ä»»ä½•å…¶ä»–æƒ…å†µ â†’ **ç«‹å³ä½¿ç”¨ search_documents å·¥å…·**

## å›ç­”è¦æ±‚
- ä¼˜å…ˆä½¿ç”¨å·¥å…·æ£€ç´¢ï¼ŒåŸºäºæ£€ç´¢ç»“æœå›ç­”
- å¦‚æœæ£€ç´¢ç»“æœä¸ç›¸å…³ï¼Œå†è€ƒè™‘ä½¿ç”¨é€šç”¨çŸ¥è¯†è¡¥å……
- ç»“åˆé•¿æœŸè®°å¿†ï¼Œæä¾›ä¸ªæ€§åŒ–çš„å›ç­”
- å®å¯å¤šæ£€ç´¢ï¼Œä¸è¦å‡­è®°å¿†ç¼–é€ """
        else:
            # æ²¡æœ‰æ–‡ä»¶IDï¼Œä¸åŠ è½½æ–‡æ¡£æ£€ç´¢å·¥å…·
            logger.info("æœªæŒ‡å®šæ–‡ä»¶IDï¼Œä¸åŠ è½½æ–‡æ¡£æ£€ç´¢å·¥å…·")
        
        # ä½¿ç”¨ FunctionAgentï¼ˆå³ä½¿æ²¡æœ‰å·¥å…·ä¹Ÿå¯ä»¥ä½¿ç”¨ï¼Œä»¥ä¾¿æ”¯æŒ memoryï¼‰
        agent = FunctionAgent(
            name="chat_agent_with_memory",
            tools=tools,
            llm=Settings.llm,
            system_prompt=system_prompt
        )
        
        # ä½¿ç”¨ Mem0 è®°å¿†è¿›è¡Œå¯¹è¯
        if memory:
            logger.info(f"ä½¿ç”¨ Mem0 è®°å¿†è¿›è¡Œå¯¹è¯ï¼ˆç”¨æˆ·: {user_id}ï¼Œå·¥å…·æ•°: {len(tools)}ï¼‰")
            handler = agent.run(user_msg=message, memory=memory)
        else:
            logger.warning(f"Mem0 è®°å¿†åˆ›å»ºå¤±è´¥ï¼Œä½¿ç”¨ç©ºèŠå¤©å†å²ï¼ˆç”¨æˆ·: {user_id}ï¼Œå·¥å…·æ•°: {len(tools)}ï¼‰")
            # å¦‚æœ Mem0 åˆ›å»ºå¤±è´¥ï¼Œä½¿ç”¨ç©ºçš„ chat_history
            handler = agent.run(user_msg=message, chat_history=[])
        
        output = await handler
        
        # output æ˜¯ AgentOutput å¯¹è±¡ï¼Œè¿”å› output å’Œ source_nodes
        return output, source_nodes


# å•ä¾‹å®ä¾‹ï¼ˆä¾èµ–æ³¨å…¥æ¨¡å¼ï¼‰
from .vector_store import get_vector_store_service

_agent_service: Optional['AgentService'] = None

def get_agent_service() -> 'AgentService':
    """
    è·å– AgentService å•ä¾‹ï¼ˆä¾èµ–æ³¨å…¥æ¨¡å¼ï¼‰
    
    ç‰¹æ€§ï¼š
    - å»¶è¿Ÿåˆå§‹åŒ–ï¼šåªåœ¨é¦–æ¬¡ä½¿ç”¨æ—¶åˆ›å»º
    - è‡ªåŠ¨ä¾èµ–ç®¡ç†ï¼šè‡ªåŠ¨è·å– VectorStoreService
    - å•ä¾‹æ¨¡å¼ï¼šåº”ç”¨ç”Ÿå‘½å‘¨æœŸå†…åªæœ‰ä¸€ä¸ªå®ä¾‹
    - æ˜“äºæµ‹è¯•ï¼šå¯ä»¥é€šè¿‡ FastAPI dependency_overrides æ›¿æ¢
    """
    global _agent_service
    if _agent_service is None:
        vector_store = get_vector_store_service()
        _agent_service = AgentService(vector_store)
    return _agent_service
