from llama_index.core import Settings
from llama_index.core.vector_stores import MetadataFilters, MetadataFilter, FilterOperator, FilterCondition
from llama_index.core.tools import FunctionTool
from llama_index.core.agent.workflow import FunctionAgent
from llama_index.core.llms import ChatMessage, MessageRole
from llama_index.memory.mem0 import Mem0Memory
from typing import List, Dict, Optional

from ..logger import logger
from .vector_store import VectorStoreService
from ..config import settings


class AgentService:
    """æ™ºèƒ½ä»£ç†æœåŠ¡ - è´Ÿè´£å¤„ç†å¯¹è¯å’ŒæŸ¥è¯¢"""
    
    def __init__(self, vector_store_service: VectorStoreService):
        self.vector_store_service = vector_store_service
        self.last_source_nodes = []  # ä¿å­˜æœ€åä¸€æ¬¡æŸ¥è¯¢çš„æºèŠ‚ç‚¹
        self._mem0_memories = {}  # ç¼“å­˜ä¸åŒç”¨æˆ·çš„è®°å¿†å®ä¾‹
    
    def _get_or_create_memory(self, user_id: str) -> Mem0Memory:
        """
        è·å–æˆ–åˆ›å»ºç”¨æˆ·çš„ Mem0 è®°å¿†å®ä¾‹
        
        æ³¨æ„ï¼šMem0 OSS æ¨¡å¼ç›®å‰å¯èƒ½ä¸å®Œå…¨æ”¯æŒè‡ªå®šä¹‰ OpenAI endpoint (å¦‚ Azure OpenAI)
        å¦‚æœä½ ä½¿ç”¨çš„æ˜¯è‡ªå®šä¹‰ endpointï¼Œå»ºè®®ï¼š
        1. ä½¿ç”¨ Mem0 Platform (è®¾ç½® MEM0_API_KEY ç¯å¢ƒå˜é‡)
        2. æˆ–è€…åœ¨é¡¹ç›®ä¸­ç¦ç”¨è®°å¿†åŠŸèƒ½
        """
        if user_id not in self._mem0_memories:
            try:
                context = {"user_id": user_id}
                
                # å¦‚æœé…ç½®äº† Mem0 Platform API Keyï¼Œä½¿ç”¨ Platform æ¨¡å¼
                if settings.MEM0_API_KEY:
                    logger.info(f"ä¸ºç”¨æˆ· {user_id} åˆ›å»º Mem0 Platform è®°å¿†å®ä¾‹")
                    self._mem0_memories[user_id] = Mem0Memory.from_client(
                        context=context,
                        api_key=settings.MEM0_API_KEY,
                        search_msg_limit=settings.MEM0_SEARCH_MSG_LIMIT,
                    )
                    logger.info(f"âœ… æˆåŠŸä¸ºç”¨æˆ· {user_id} åˆ›å»º Mem0 Platform è®°å¿†å®ä¾‹")
                else:
                    # OSS æ¨¡å¼ï¼šé€šè¿‡ç¯å¢ƒå˜é‡é…ç½®
                    logger.info(f"å°è¯•ä¸ºç”¨æˆ· {user_id} åˆ›å»º Mem0 OSS è®°å¿†å®ä¾‹")
                    logger.warning("âš ï¸  Mem0 éœ€è¦é€šè¿‡ç¯å¢ƒå˜é‡è®¿é—® OpenAI API")
                    logger.warning("âš ï¸  å¦‚æœä½ ä½¿ç”¨ Azure OpenAI æˆ–å…¶ä»–è‡ªå®šä¹‰ endpointï¼Œå»ºè®®ä½¿ç”¨ Mem0 Platform")
                    
                    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆMem0 ä¼šè‡ªåŠ¨è¯»å–ï¼‰
                    import os
                    os.environ["OPENAI_API_KEY"] = settings.OPENAI_API_KEY
                    if settings.OPENAI_API_BASE:
                        # Mem0 æ¨èä½¿ç”¨ OPENAI_BASE_URL (æ–°ç‰ˆæœ¬)ï¼Œä¹Ÿæ”¯æŒ OPENAI_API_BASE (æ—§ç‰ˆæœ¬)
                        os.environ["OPENAI_BASE_URL"] = settings.OPENAI_API_BASE
                        os.environ["OPENAI_API_BASE"] = settings.OPENAI_API_BASE  # å…¼å®¹æ—§ç‰ˆæœ¬
                        logger.info(f"   è®¾ç½® OPENAI_BASE_URL: {settings.OPENAI_API_BASE}")
                    
                    mem0_config = {
                        "vector_store": {
                            "provider": "qdrant",
                            "config": {
                                "collection_name": f"mem0_{user_id}",
                                "host": settings.QDRANT_HOST,
                                "port": settings.QDRANT_PORT,
                                "embedding_model_dims": 1536,
                            },
                        },
                        "llm": {
                            "provider": "openai",
                            "config": {
                                "model": settings.OPENAI_MODEL,
                                "temperature": 0.2,
                                "max_tokens": 1500,
                                # ä¸è¦åœ¨è¿™é‡Œè®¾ç½® api_key å’Œ base_url
                                # Mem0 ä¼šè‡ªåŠ¨ä»ç¯å¢ƒå˜é‡è¯»å–
                            },
                        },
                        "embedder": {
                            "provider": "openai",
                            "config": {
                                "model": settings.OPENAI_EMBEDDING_MODEL,
                                # ä¸è¦åœ¨è¿™é‡Œè®¾ç½® api_key å’Œ base_url
                                # Mem0 ä¼šè‡ªåŠ¨ä»ç¯å¢ƒå˜é‡è¯»å–
                            },
                        },
                        "version": "v1.1",
                    }
                    self._mem0_memories[user_id] = Mem0Memory.from_config(
                        context=context,
                        config=mem0_config,
                        search_msg_limit=settings.MEM0_SEARCH_MSG_LIMIT,
                    )
                    logger.info(f"âœ… æˆåŠŸä¸ºç”¨æˆ· {user_id} åˆ›å»º Mem0 OSS è®°å¿†å®ä¾‹")
            except Exception as e:
                logger.error(f"âŒ åˆ›å»º Mem0 è®°å¿†å¤±è´¥: {e}")
                logger.error(f"   è®°å¿†åŠŸèƒ½å°†è¢«ç¦ç”¨ï¼Œç³»ç»Ÿå°†ä½¿ç”¨ä¼ ç»Ÿçš„èŠå¤©å†å²")
                logger.error(f"   å»ºè®®ï¼š")
                logger.error(f"   1. ä½¿ç”¨ Mem0 Platform (è®¾ç½® MEM0_API_KEY)")
                logger.error(f"   2. æˆ–ä½¿ç”¨æ ‡å‡†çš„ OpenAI API (ä¸ä½¿ç”¨è‡ªå®šä¹‰ endpoint)")
                # å¦‚æœå¤±è´¥ï¼Œç¼“å­˜ Noneï¼Œé¿å…é‡å¤å°è¯•
                self._mem0_memories[user_id] = None
                return None
        
        return self._mem0_memories.get(user_id)
    
    async def query(self, query_text: str, chat_history: List[Dict], file_ids: Optional[List[str]] = None, top_k: int = 3, user_id: str = "default_user"):
        """ä½¿ç”¨ FunctionAgent è¿›è¡Œå¯¹è¯æŸ¥è¯¢ï¼Œé›†æˆ Mem0 è®°å¿†æ¨¡å—"""
        if not self.vector_store_service.index:
            await self.vector_store_service.initialize()
            
        filters = None
        if file_ids:
            filters = MetadataFilters(
                filters=[
                    MetadataFilter(key="file_id", value=fid)
                    for fid in file_ids
                ],
                condition=FilterCondition.OR,
            )
        
        # è·å–æˆ–åˆ›å»ºè¯¥ç”¨æˆ·çš„ Mem0 è®°å¿†å®ä¾‹
        memory = self._get_or_create_memory(user_id)
        
        # å°†å†å²è®°å½•è½¬æ¢ä¸º LlamaIndex çš„ ChatMessage å¯¹è±¡
        messages = []
        for msg in chat_history:
            role = MessageRole.USER if msg.role == "user" else MessageRole.ASSISTANT
            messages.append(ChatMessage(role=role, content=msg.content))
            
        # åˆ›å»ºæŸ¥è¯¢å¼•æ“å·¥å…·
        query_engine = self.vector_store_service.index.as_query_engine(
            similarity_top_k=top_k,
            filters=filters
        )
        
        async def search_documents(query: str):
            """Useful for answering natural language questions about uploaded documents."""
            logger.info(f"Agentè°ƒç”¨æœç´¢å·¥å…·ï¼ŒæŸ¥è¯¢å†…å®¹: {query}")
            response = await query_engine.aquery(query)
            logger.info(f"æœç´¢å·¥å…·è¿”å›ç»“æœ: {str(response)[:200]}... (Total len: {len(str(response))})")
            
            # ä¿å­˜æºèŠ‚ç‚¹ä¾›åç»­ä½¿ç”¨
            if hasattr(response, 'source_nodes'):
                self.last_source_nodes = response.source_nodes
                logger.info(f"æœç´¢åˆ° {len(response.source_nodes)} ä¸ªç›¸å…³ç‰‡æ®µ")
                for i, node in enumerate(response.source_nodes):
                    logger.info(f"  [ç‰‡æ®µ {i+1}] Score: {node.score:.4f}, File: {node.metadata.get('filename')}")
                    logger.info(f"  Content: {node.text[:100]}...")  # æ‰“å°ç‰‡æ®µå†…å®¹å‰100å­—ç¬¦
            else:
                self.last_source_nodes = []
            
            # è¿”å›å­—ç¬¦ä¸²ç»™LLMï¼Œä½†æºèŠ‚ç‚¹å·²ä¿å­˜
            return str(response)

        query_tool = FunctionTool.from_defaults(
            async_fn=search_documents,
            name="search_documents",
            description="""ä»å·²ä¸Šä¼ çš„æ–‡æ¡£ä¸­æ£€ç´¢ä¿¡æ¯ã€‚

**å¿…é¡»ä½¿ç”¨æ­¤å·¥å…·çš„æƒ…å†µï¼š**
- ç”¨æˆ·æ˜ç¡®è¯¢é—®æ–‡æ¡£å†…å®¹ï¼ˆå¦‚"æ–‡æ¡£é‡Œè¯´äº†ä»€ä¹ˆ"ã€"æ€»ç»“è¿™ä¸ªPDF"ï¼‰
- ç”¨æˆ·è¯¢é—®ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼ˆå¯èƒ½åœ¨æ–‡æ¡£ä¸­ï¼‰
- ç”¨æˆ·è¦æ±‚å¼•ç”¨ã€æŸ¥æ‰¾æˆ–éªŒè¯å…·ä½“ä¿¡æ¯
- ç”¨æˆ·è¯¢é—®é¡¹ç›®ã€äº§å“ã€æŠ€æœ¯æ–‡æ¡£ä¸­çš„ç»†èŠ‚

**ä¸è¦ä½¿ç”¨æ­¤å·¥å…·çš„æƒ…å†µï¼š**
- æ—¥å¸¸ç”Ÿæ´»é—®é¢˜ï¼ˆå¦‚"ä»Šå¤©å¤©æ°”"ã€"å¦‚ä½•åšé¥­"ï¼‰
- å¸¸è¯†æ€§é—®é¢˜ï¼ˆå¦‚"åœ°çƒæœ‰å¤šå¤§"ã€"ä»€ä¹ˆæ˜¯é‡åŠ›"ï¼‰
- é—²èŠå’Œé—®å€™ï¼ˆå¦‚"ä½ å¥½"ã€"æœ€è¿‘æ€ä¹ˆæ ·"ï¼‰
- é€šç”¨å»ºè®®ï¼ˆå¦‚"å¦‚ä½•å­¦ä¹ ç¼–ç¨‹"ã€"å¦‚ä½•é”»ç‚¼èº«ä½“"ï¼‰

**åˆ¤æ–­åŸåˆ™ï¼š** å¦‚æœé—®é¢˜çš„ç­”æ¡ˆå¯èƒ½åœ¨ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£ä¸­ï¼Œåˆ™å¿…é¡»ä½¿ç”¨æ­¤å·¥å…·ï¼›å¦‚æœæ˜¯æ™®é€šå¸¸è¯†æˆ–ç”Ÿæ´»é—®é¢˜ï¼Œç›´æ¥å›ç­”å³å¯ã€‚"""
        )
        
        # å¢å¼ºçš„ç³»ç»Ÿæç¤ºï¼Œç»“åˆè®°å¿†å’ŒçŸ¥è¯†åº“
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œæ‹¥æœ‰é•¿æœŸè®°å¿†å’Œæ–‡æ¡£æ£€ç´¢èƒ½åŠ›ã€‚

## ä½ çš„èƒ½åŠ›

1. **é•¿æœŸè®°å¿†** - è‡ªåŠ¨è®°ä½ç”¨æˆ·çš„åå¥½ã€èƒŒæ™¯ä¿¡æ¯
2. **æ–‡æ¡£æ£€ç´¢** - ä½¿ç”¨ search_documents å·¥å…·æŸ¥è¯¢ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£
3. **å¸¸è¯†å›ç­”** - ç›´æ¥å›ç­”æ—¥å¸¸é—®é¢˜å’Œå¸¸è¯†æ€§é—®é¢˜

## å·¥å…·ä½¿ç”¨ç­–ç•¥

### âœ… å¿…é¡»ä½¿ç”¨ search_documents å·¥å…·ï¼š
- ç”¨æˆ·æ˜ç¡®æåˆ°"æ–‡æ¡£"ã€"PDF"ã€"ä¸Šä¼ çš„èµ„æ–™"ç­‰
- è¯¢é—®ä¸“ä¸šé¢†åŸŸçŸ¥è¯†ï¼ˆå¯èƒ½åœ¨æ–‡æ¡£ä¸­ï¼‰
- éœ€è¦å¼•ç”¨å…·ä½“æ•°æ®ã€è§‚ç‚¹ã€ç»†èŠ‚
- ä¾‹å¦‚ï¼š"æ–‡æ¡£ä¸­æåˆ°çš„æ–¹æ¡ˆæ˜¯ä»€ä¹ˆï¼Ÿ"ã€"æ€»ç»“ä¸€ä¸‹è¿™ä¸ªæŠ¥å‘Š"

### âŒ ä¸è¦ä½¿ç”¨ search_documents å·¥å…·ï¼š
- æ—¥å¸¸ç”Ÿæ´»é—®é¢˜ï¼š"ä»Šå¤©åƒä»€ä¹ˆ"ã€"å¦‚ä½•é”»ç‚¼"
- é€šç”¨å¸¸è¯†ï¼š"ä»€ä¹ˆæ˜¯Python"ã€"åœ°çƒæœ‰å¤šå¤§"
- é—²èŠé—®å€™ï¼š"ä½ å¥½"ã€"æœ€è¿‘æ€ä¹ˆæ ·"
- é€šç”¨å»ºè®®ï¼š"æ¨èä¸€æœ¬ä¹¦"ã€"å¦‚ä½•å­¦ä¹ "

### ğŸ¤” åˆ¤æ–­åŸåˆ™ï¼š
**é—®è‡ªå·±ï¼šè¿™ä¸ªé—®é¢˜çš„ç­”æ¡ˆå¯èƒ½åœ¨ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£ä¸­å—ï¼Ÿ**
- æ˜¯ â†’ ä½¿ç”¨ search_documents å·¥å…·
- å¦ â†’ ç›´æ¥ç”¨ä½ çš„çŸ¥è¯†å›ç­”

## å›ç­”è¦æ±‚
- ä½¿ç”¨å·¥å…·æ—¶ï¼ŒåŸºäºæ£€ç´¢ç»“æœå›ç­”ï¼Œä¸è¦ç¼–é€ 
- ä¸ä½¿ç”¨å·¥å…·æ—¶ï¼Œè‡ªä¿¡åœ°ç”¨å¸¸è¯†å›ç­”
- ç»“åˆé•¿æœŸè®°å¿†ï¼Œæä¾›ä¸ªæ€§åŒ–çš„å›ç­”"""
        
        # ä½¿ç”¨ FunctionAgentï¼Œé›†æˆ Mem0 è®°å¿†
        agent = FunctionAgent(
            name="rag_agent_with_memory",
            tools=[query_tool],
            llm=Settings.llm,
            system_prompt=system_prompt
        )
        
        # å¦‚æœæœ‰ memoryï¼Œåˆ™ä¼ å…¥ï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤çš„ chat_history
        if memory:
            logger.info(f"ä½¿ç”¨ Mem0 è®°å¿†è¿›è¡Œå¯¹è¯ï¼ˆç”¨æˆ·: {user_id}ï¼‰")
            handler = agent.run(user_msg=query_text, memory=memory)
        else:
            logger.info(f"æœªä½¿ç”¨è®°å¿†ï¼Œä½¿ç”¨ä¼ ç»ŸèŠå¤©å†å²")
            handler = agent.run(user_msg=query_text, chat_history=messages)
        
        output = await handler
        
        # output æ˜¯ AgentOutput å¯¹è±¡
        return output

    async def chat(self, message: str, chat_history: List[Dict], user_id: str = "default_user"):
        """çº¯ LLM å¯¹è¯ï¼Œä¸æ£€ç´¢å‘é‡åº“ï¼Œä½†å¯ä»¥ä½¿ç”¨ Mem0 è®°å¿†"""
        # è·å–æˆ–åˆ›å»ºè¯¥ç”¨æˆ·çš„ Mem0 è®°å¿†å®ä¾‹
        memory = self._get_or_create_memory(user_id)
        
        if memory:
            # ä½¿ç”¨ FunctionAgent ä»¥æ”¯æŒ memoryï¼ˆå³ä½¿æ²¡æœ‰å·¥å…·ï¼‰
            logger.info(f"ä½¿ç”¨ Mem0 è®°å¿†è¿›è¡Œçº¯ LLM å¯¹è¯ï¼ˆç”¨æˆ·: {user_id}ï¼‰")
            agent = FunctionAgent(
                name="chat_agent_with_memory",
                tools=[],  # ä¸æä¾›å·¥å…·
                llm=Settings.llm,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„æ™ºèƒ½åŠ©æ‰‹ã€‚ä½ èƒ½è®°ä½ç”¨æˆ·çš„åå¥½å’Œè¿‡å¾€å¯¹è¯ä¿¡æ¯ï¼Œæä¾›ä¸ªæ€§åŒ–çš„æœåŠ¡ã€‚"
            )
            handler = agent.run(user_msg=message, memory=memory)
            output = await handler
            return output.response.content if hasattr(output, 'response') else str(output)
        else:
            # æ²¡æœ‰è®°å¿†ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹å¼
            messages = []
            for msg in chat_history:
                role = MessageRole.USER if msg.role == "user" else MessageRole.ASSISTANT
                messages.append(ChatMessage(role=role, content=msg.content))
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            messages.append(ChatMessage(role=MessageRole.USER, content=message))
                
            # ä½¿ç”¨é…ç½®å¥½çš„ LLM ç›´æ¥å›ç­”
            response = await Settings.llm.achat(messages)
            return response.message.content


# å…¨å±€å®ä¾‹
from .vector_store import vector_store_service
agent_service = AgentService(vector_store_service)
